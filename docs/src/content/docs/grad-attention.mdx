---
title: "11. Gradients: Attention"
description: Complete backpropagation through multi-head attention including softmax Jacobian
---

We're almost there.

We've backpropagated through the output layer, layer norm, and FFN. Now we need to tackle the most complex part: **multi-head attention**.

Attention is intricate. It involves:
- Output projection ($W_o$)
- Concatenation of heads
- Attention mechanism (softmax, attention scores, $QK^T$)
- Query/Key/Value projections ($W_q$, $W_k$, $W_v$)

Each of these needs gradients. It's a lot.

## The Challenge

Attention gradients are mathematically complex because:

1. **Softmax** creates dependencies between all positions (requires Jacobian)
2. **Matrix multiplications** create cross-dependencies between dimensions
3. **Causal masking** affects gradient flow asymmetrically
4. **Multiple heads** mean we're doing this several times in parallel

In practice, deep learning frameworks (PyTorch, TensorFlow) handle all this automatically. But for this educational project, we'll compute **the complete, accurate gradients** to truly understand what's happening.

## Complete Backward Pass Through Attention

Let's work backward through each operation. We start with $\frac{\partial L}{\partial \text{multi\_head\_output}}$ from the previous layer.

### Step 1: Backprop Through Output Projection

**Forward pass:**
$$
\text{multi\_head\_output} = \text{concat} \cdot W_o^T
$$

where concat is the concatenation of all head outputs.

**Backward pass:**
$$
\frac{\partial L}{\partial \text{concat}}[pos] = W_o \cdot \frac{\partial L}{\partial \text{multi\_head\_output}}[pos]
$$

$$
\frac{\partial L}{\partial W_o} = \sum_{pos} \frac{\partial L}{\partial \text{multi\_head\_output}}[pos]^T \otimes \text{concat}[pos]
$$

### Step 2: Split Gradient by Heads

The concatenated output was formed by concatenating the outputs from each head:
$$
\text{concat}[pos] = [\text{head}_0[pos], \text{head}_1[pos], \ldots]
$$

We split $\frac{\partial L}{\partial \text{concat}}$ back into per-head gradients:
$$
\frac{\partial L}{\partial \text{head}_h}[pos] = \frac{\partial L}{\partial \text{concat}}[pos][h \cdot d_{\text{head}} : (h+1) \cdot d_{\text{head}}]
$$

### Step 3a: Backprop Through Attention Output

For each head, the forward pass computed:
$$
\text{attention\_output}[i] = \sum_j \text{attention\_weights}[i,j] \cdot V[j]
$$

**Gradients:**

For the value matrix $V$:
$$
\frac{\partial L}{\partial V[j]} = \sum_i \text{attention\_weights}[i,j] \cdot \frac{\partial L}{\partial \text{attention\_output}[i]}
$$

For the attention weights:
$$
\frac{\partial L}{\partial \text{attention\_weights}[i,j]} = \frac{\partial L}{\partial \text{attention\_output}[i]} \cdot V[j]
$$

### Step 3b: Backprop Through Softmax â€” THE JACOBIAN

This is the key step! The forward pass computed:
$$
\text{attention\_weights}[i] = \text{softmax}(\text{scores\_masked}[i])
$$

For a softmax function $s = \text{softmax}(x)$, the **Jacobian** is:
$$
\frac{\partial s_j}{\partial x_k} = s_j \cdot (\delta_{jk} - s_k)
$$

where $\delta_{jk}$ is the Kronecker delta (1 if $j=k$, 0 otherwise).

Using the chain rule, for each position $i$:
$$
\frac{\partial L}{\partial \text{scores\_masked}[i,k]} = \sum_j \frac{\partial L}{\partial \text{attention\_weights}[i,j]} \cdot \frac{\partial \text{attention\_weights}[i,j]}{\partial \text{scores\_masked}[i,k]}
$$

This simplifies to:
$$
\frac{\partial L}{\partial \text{scores\_masked}[i,k]} = \text{attention\_weights}[i,k] \cdot \left( \frac{\partial L}{\partial \text{attention\_weights}[i,k]} - \sum_j \text{attention\_weights}[i,j] \cdot \frac{\partial L}{\partial \text{attention\_weights}[i,j]} \right)
$$

Or more compactly:
$$
\frac{\partial L}{\partial \text{scores\_masked}[i]} = s[i] \odot \left( \frac{\partial L}{\partial s[i]} - \left( s[i] \cdot \frac{\partial L}{\partial s[i]} \right) \right)
$$

where $\odot$ is element-wise multiplication and the dot represents the dot product.

### Step 3c: Backprop Through Causal Mask

The mask set future positions to $-\infty$:
$$
\text{scores\_masked}[i,j] = \begin{cases}
\text{scores\_scaled}[i,j] & \text{if } j \leq i \\
-\infty & \text{otherwise}
\end{cases}
$$

Gradients don't flow through masked positions:
$$
\frac{\partial L}{\partial \text{scores\_scaled}[i,j]} = \begin{cases}
\frac{\partial L}{\partial \text{scores\_masked}[i,j]} & \text{if } j \leq i \\
0 & \text{otherwise}
\end{cases}
$$

### Step 3d: Backprop Through Scaling

The forward pass divided by $\sqrt{d_k}$:
$$
\text{scores\_scaled} = \frac{\text{scores\_raw}}{\sqrt{d_k}}
$$

Backward:
$$
\frac{\partial L}{\partial \text{scores\_raw}} = \frac{1}{\sqrt{d_k}} \cdot \frac{\partial L}{\partial \text{scores\_scaled}}
$$

### Step 3e: Backprop Through $Q \cdot K^T$

The forward pass computed:
$$
\text{scores\_raw}[i,j] = Q[i] \cdot K[j]
$$

**Gradients:**
$$
\frac{\partial L}{\partial Q[i]} = \sum_j \frac{\partial L}{\partial \text{scores}[i,j]} \cdot K[j]
$$

$$
\frac{\partial L}{\partial K[j]} = \sum_i \frac{\partial L}{\partial \text{scores}[i,j]} \cdot Q[i]
$$

### Step 4: Backprop Through Q/K/V Projections

The forward pass computed:
$$
Q = X \cdot W_q, \quad K = X \cdot W_k, \quad V = X \cdot W_v
$$

where $W_q, W_k, W_v$ are $[d_{\text{model}}, d_{\text{head}}]$ matrices.

**Weight gradients:**
$$
\frac{\partial L}{\partial W_q} = X^T \cdot \frac{\partial L}{\partial Q} = \sum_{pos} X[pos]^T \otimes \frac{\partial L}{\partial Q[pos]}
$$

And similarly for $W_k$ and $W_v$.

**Input gradients:**
$$
\frac{\partial L}{\partial X} = \frac{\partial L}{\partial Q} \cdot W_q^T + \frac{\partial L}{\partial K} \cdot W_k^T + \frac{\partial L}{\partial V} \cdot W_v^T
$$

## Complete Implementation

The Python script `scripts/11_grad_attention_embeddings.py` implements all of these steps accurately, including the softmax Jacobian. Here are the key numerical results:

```python
# After backprop through all attention mechanisms:
dL/dV[0][0] = [ 0.0421,  0.0808,  0.0449, -0.1398, -0.0372, -0.0140,  0.0050, -0.0203]

# Gradients for weight matrices:
dL/dW_q[0][0] = [ 0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000]
dL/dW_v[0][0] = [ 0.0046,  0.0108,  0.0023, -0.0220,  0.0018, -0.0064, -0.0009, -0.0015]

# Final gradients flowing back to embeddings:
dL/dX[0] = [-0.0346,  0.0085, -0.0320,  0.0154,  0.0032,  0.0038, -0.0417,  0.0222,
             0.0074,  0.0124,  0.0262,  0.0289, -0.0004, -0.0058,  0.0050, -0.0377]
```

## The Takeaway

Attention backpropagation is complex, but it's **completely computable** using the chain rule systematically applied through each operation.

The key insights:

1. **Softmax Jacobian** is the most complex part, creating dependencies between all positions in each row
2. **Matrix multiplications** backprop cleanly using transposes and outer products
3. **Causal mask** blocks gradient flow to future positions
4. **Everything compounds** through multiple heads and positions

We've now computed the **complete, accurate gradients** for attention, with no approximations. This is exactly what PyTorch's autograd computes internally (though with more efficient implementations).

## What's Next

We've made it through attention. One final step remains:

**Embeddings.**

The gradients we just computed ($\frac{\partial L}{\partial X}$) tell us how the loss changes with respect to the input embeddings. Now we need to split these gradients between:
- Token embeddings ($E_{token}$)
- Position embeddings ($E_{pos}$)

Then we'll have gradients for every single parameter in the model.

Almost done.
