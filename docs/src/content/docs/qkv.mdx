---
title: "2. QKV Projections"
description: Transforming embeddings into Query, Key, and Value representations
---

After obtaining our embedding matrix `X`, the next step in the transformer is to compute **Query (Q)**, **Key (K)**, and **Value (V)** projections. These are the fundamental building blocks of the attention mechanism.

## What are Q, K, V?

The attention mechanism allows the model to determine how much "attention" each token should pay to every other token. This is done through three different representations:

- **Query (Q)**: "What am I looking for?"
- **Key (K)**: "What do I contain?"
- **Value (V)**: "What information do I have?"

Each token's embedding is projected into these three representations using learned weight matrices.

## Multi-Head Attention Structure

Our model uses **multi-head attention** with `num_heads = 2`. This means we compute attention independently in 2 different subspaces, then combine the results.

**Architecture:**
- **d_model:** 16 (embedding dimension)
- **num_heads:** 2
- **d_k:** d_model / num_heads = 8 (dimension per head)

Each head has its own set of weight matrices that project the 16-dimensional embeddings into 8-dimensional Q, K, V representations.

## Matrix Shape Notation

Throughout this documentation, when we write a matrix shape as `[m, n]`, this means:
- **m** = number of rows
- **n** = number of columns

For example, a matrix with shape `[5, 16]` has 5 rows and 16 columns. This is important to keep in mind when performing matrix multiplication!

## Weight Matrices

For each head, we need three weight matrices:

**For Head 0:**
- **W_Q[0]:** Query weight matrix `[16, 8]` (16 rows, 8 columns)
- **W_K[0]:** Key weight matrix `[16, 8]` (16 rows, 8 columns)
- **W_V[0]:** Value weight matrix `[16, 8]` (16 rows, 8 columns)

**For Head 1:**
- **W_Q[1]:** Query weight matrix `[16, 8]` (16 rows, 8 columns)
- **W_K[1]:** Key weight matrix `[16, 8]` (16 rows, 8 columns)
- **W_V[1]:** Value weight matrix `[16, 8]` (16 rows, 8 columns)

These matrices are initialized with small random values and are learned during training.

**In Python:**

```python
# Initialize weight matrices for each head
W_Q = []
W_K = []
W_V = []

for head in range(NUM_HEADS):
    W_Q.append(random_matrix(D_MODEL, D_K))  # [16, 8]
    W_K.append(random_matrix(D_MODEL, D_K))  # [16, 8]
    W_V.append(random_matrix(D_MODEL, D_K))  # [16, 8]
```

## Matrix Multiplication Basics

Before we compute Q, K, V, let's review **matrix multiplication** since it's the core operation we'll be using.

:::note[The @ Symbol]
The `@` symbol represents **matrix multiplication** (not dot product). In Python and NumPy, `A @ B` performs matrix multiplication between A and B. This is equivalent to `np.matmul(A, B)` or `np.dot(A, B)` for 2D matrices.
:::

### How Matrix Multiplication Works

When we multiply two matrices `A @ B`:
- **A** has shape `[m, n]` (m rows, n columns)
- **B** has shape `[n, p]` (n rows, p columns)
- The result has shape `[m, p]` (m rows, p columns)

**Key requirement:** The number of columns in A must equal the number of rows in B!

**The operation:** To compute element `[i, j]` in the result:
```
result[i, j] = sum of (A[i, k] * B[k, j]) for all k
```

In other words, we take row `i` from A, column `j` from B, multiply corresponding elements, and sum them up. This is actually computing the **dot product** between row `i` of A and column `j` of B! So matrix multiplication is really a collection of many dot products organized into a matrix.

### Simple Example

Let's multiply a `[2, 3]` matrix by a `[3, 2]` matrix:

```
A = [                B = [
  [1, 2, 3]            [1, 4]
  [4, 5, 6]            [2, 5]
]                      [3, 6]
                     ]

Result = A @ B  (will be [2, 2])

Result[0, 0] = (1*1) + (2*2) + (3*3) = 1 + 4 + 9 = 14
Result[0, 1] = (1*4) + (2*5) + (3*6) = 4 + 10 + 18 = 32
Result[1, 0] = (4*1) + (5*2) + (6*3) = 4 + 10 + 18 = 32
Result[1, 1] = (4*4) + (5*5) + (6*6) = 16 + 25 + 36 = 77

Result = [
  [14, 32]
  [32, 77]
]
```

### Applying to Our Embeddings

When we compute `X @ W_Q[head]`:
- **X** is `[5, 16]` - each of our 5 tokens has a 16-dimensional embedding
- **W_Q[head]** is `[16, 8]` - transforms 16 dimensions to 8 dimensions
- **Result** is `[5, 8]` - each of our 5 tokens now has an 8-dimensional query vector

Each row of the result is computed by taking one row from X (one token's embedding) and multiplying it by the entire W_Q matrix.

## The Projection Operation

Now we can compute Q, K, V for each head using matrix multiplication:

```
Q[head] = X @ W_Q[head]
K[head] = X @ W_K[head]
V[head] = X @ W_V[head]
```

Where:
- `X` has shape `[seq_len, d_model]` = `[5, 16]` (5 rows, 16 columns)
- `W_Q[head]`, `W_K[head]`, `W_V[head]` have shape `[d_model, d_k]` = `[16, 8]` (16 rows, 8 columns)
- `Q[head]`, `K[head]`, `V[head]` have shape `[seq_len, d_k]` = `[5, 8]` (5 rows, 8 columns)

Each row of the result represents the Q/K/V vector for one token position.

## Example Calculation: Query for Position 0, Head 0

Let's walk through computing `Q[0][0]` (the query vector for the first token `<BOS>` in head 0).

**Input embedding X[0]:**
```python
X[0] = [0.1473, 0.1281, 0.1995, -0.0465, 0.2125, -0.1338, -0.0829, -0.0638,
        0.0722, 0.1183, 0.1193, 0.0937, -0.1594, -0.0402, 0.1124, -0.2064]
```

**Weight matrix W_Q[0]** (showing as rows, but we use it as columns in multiplication):
```python
W_Q[0] = [
  [-0.0144, -0.0173, -0.0111,  0.0702, -0.0128, -0.1497,  0.0332, -0.0267],  # dim 0
  [-0.0217,  0.0116,  0.0232,  0.1164,  0.0657,  0.0111, -0.0738, -0.1015],  # dim 1
  [ 0.0246,  0.1311,  0.0042, -0.0106,  0.0532, -0.1454, -0.0312,  0.0490],  # dim 2
  ...  # (16 rows total, one for each input dimension)
]
```

**Matrix multiplication Q[0][0] = X[0] @ W_Q[0]:**

For each output dimension `j` (0 to 7), we compute:
```
Q[0][0][j] = sum(X[0][i] * W_Q[0][i][j] for i in range(16))
```

**Example for dimension 0:**
```python
Q[0][0][0] = (0.1473 * -0.0144) + (0.1281 * -0.0217) + (0.1995 * 0.0246) + ...
           = -0.0783
```

**Complete result:**
```python
Q[0][0] = [-0.0783, 0.0149, -0.0577, 0.0237, -0.0221, -0.0892, -0.0084, 0.0240]
```

This 8-dimensional vector is the query representation for `<BOS>` in head 0!

## Head 0 Results

### Query Matrix Q[0]

Each token's embedding is projected into an 8-dimensional query vector:

```python
Q[0] = [
  [-0.0783,  0.0149, -0.0577,  0.0237, -0.0221, -0.0892, -0.0084,  0.0240],  # pos 0: <BOS>
  [ 0.0389, -0.0146,  0.0477, -0.0396,  0.0275,  0.0247,  0.0317,  0.0619],  # pos 1: I
  [ 0.0059, -0.0575, -0.0261, -0.0115,  0.0472, -0.0096,  0.0411,  0.0206],  # pos 2: like
  [-0.0523, -0.0658, -0.0201, -0.0225, -0.0305,  0.1527,  0.0317, -0.0174],  # pos 3: transformers
  [-0.0249, -0.0083, -0.0226, -0.0095, -0.0193,  0.0178,  0.0186, -0.0663],  # pos 4: <EOS>
]
```

### Key Matrix K[0]

Each token's embedding is projected into an 8-dimensional key vector:

```python
K[0] = [
  [ 0.0237,  0.0403, -0.0276,  0.0297, -0.0048, -0.0394,  0.0016,  0.0661],  # pos 0: <BOS>
  [-0.0188,  0.0668, -0.0556, -0.0324, -0.0575,  0.1045,  0.0703, -0.0518],  # pos 1: I
  [ 0.0190,  0.0061, -0.0610,  0.0896,  0.0995, -0.0345,  0.1541,  0.0085],  # pos 2: like
  [-0.0224,  0.0178,  0.0351,  0.0453, -0.0398, -0.0445, -0.0057, -0.0410],  # pos 3: transformers
  [-0.0921,  0.0558,  0.0060,  0.0653, -0.0376,  0.0249, -0.0021,  0.0258],  # pos 4: <EOS>
]
```

### Value Matrix V[0]

Each token's embedding is projected into an 8-dimensional value vector:

```python
V[0] = [
  [ 0.0231, -0.0259, -0.0087, -0.0535,  0.0329, -0.0185, -0.0107, -0.0314],  # pos 0: <BOS>
  [ 0.0223, -0.0442,  0.0885,  0.0062,  0.0939,  0.0013,  0.0087, -0.0069],  # pos 1: I
  [ 0.0201, -0.0299,  0.0548,  0.0787,  0.0212, -0.0411, -0.0330,  0.1308],  # pos 2: like
  [ 0.0407,  0.1356, -0.0384,  0.0583, -0.0175, -0.1173,  0.0662, -0.0832],  # pos 3: transformers
  [ 0.0119,  0.0191, -0.0657, -0.0651, -0.1262, -0.0073,  0.0189, -0.0118],  # pos 4: <EOS>
]
```

## Head 1 Results

The second head uses different weight matrices, producing different Q, K, V representations:

### Query Matrix Q[1]

```python
Q[1] = [
  [-0.0487,  0.0306, -0.0214,  0.0027, -0.0770,  0.0350,  0.0516,  0.0025],  # pos 0: <BOS>
  [-0.0186, -0.0149, -0.0263,  0.0499,  0.0179, -0.1268,  0.0015,  0.0944],  # pos 1: I
  [ 0.0442, -0.0715,  0.0168, -0.0637,  0.1023,  0.0303,  0.0160,  0.0567],  # pos 2: like
  [ 0.0495,  0.0629,  0.0130,  0.0490, -0.0238,  0.0603, -0.0942,  0.0047],  # pos 3: transformers
  [-0.0117, -0.0137,  0.0168, -0.0555, -0.0604,  0.0532, -0.0543,  0.0284],  # pos 4: <EOS>
]
```

### Key Matrix K[1]

```python
K[1] = [
  [-0.0040,  0.0072, -0.0243,  0.0326, -0.0827, -0.0121,  0.0501, -0.0767],  # pos 0: <BOS>
  [ 0.0688,  0.0622,  0.0367,  0.0310,  0.0217,  0.1512, -0.0243, -0.1020],  # pos 1: I
  [-0.1357, -0.0510,  0.0142, -0.0069,  0.0341, -0.0219,  0.0225,  0.0624],  # pos 2: like
  [-0.0146,  0.0500, -0.0833,  0.0250,  0.0395, -0.0145,  0.0793,  0.0859],  # pos 3: transformers
  [-0.0348,  0.0530, -0.0397,  0.0452,  0.0053, -0.1379,  0.0541,  0.0430],  # pos 4: <EOS>
]
```

### Value Matrix V[1]

```python
V[1] = [
  [ 0.1121,  0.0715,  0.0069, -0.0306,  0.1259, -0.0195,  0.0731, -0.0461],  # pos 0: <BOS>
  [ 0.0024,  0.0298,  0.0502,  0.0331,  0.0520,  0.0361, -0.0446,  0.0363],  # pos 1: I
  [ 0.0249, -0.0945, -0.0481, -0.0826, -0.1611, -0.1208, -0.0112, -0.0258],  # pos 2: like
  [-0.0106,  0.0512, -0.0149, -0.0264,  0.0692, -0.0507, -0.0323, -0.0334],  # pos 3: transformers
  [ 0.0152,  0.0116,  0.0093, -0.0667, -0.0189, -0.0389, -0.0596,  0.0296],  # pos 4: <EOS>
]
```

## Why Multiple Heads?

Notice that Head 0 and Head 1 produce different Q, K, V representations for the same input. This is the power of multi-head attention:

- **Head 0** might learn to focus on syntactic relationships
- **Head 1** might learn to focus on semantic relationships

By having multiple heads, the model can attend to different types of relationships simultaneously. Later, we'll combine the outputs from both heads to get a richer representation.

## What's Next

Now that we have Q, K, V for both heads, we can compute:
1. **Attention scores**: How much should each token attend to every other token?
2. **Attention weights**: Normalized scores (using softmax)
3. **Attention output**: Weighted combination of value vectors

Let's move on to computing attention scores!
