---
title: "10. Gradients: FFN & Layer Norm"
description: Backpropagating through the feed-forward network, residual connections, and layer normalization
---

Alright. We're continuing our backward journey through the transformer.

We've got gradients for the layer norm output. Now we need to backpropagate through:
1. **Layer normalization** (mean/variance normalization)
2. **Residual connection** (addition splits gradients)
3. **Feed-forward network** (two linear layers + GELU)

This is where things get interesting. Multiple operations, complex dependencies, gradient paths splitting and merging.

Let's work through it.

## The Architecture (Review)

In the forward pass, we computed:

```
multi_head_output  →  FFN  →  ffn_output
        ↓                         ↓
        └────────(+)──────────→ residual_output  →  LayerNorm  →  layer_norm_output
```

Now we're going backward. We have $\frac{\partial L}{\partial \text{layer\_norm\_output}}$ and need $\frac{\partial L}{\partial \text{multi\_head\_output}}$.

## Step 1: Backward Through Layer Normalization

Layer normalization is mathematically complex to backpropagate through because every output depends on **all** inputs (due to the mean and variance calculations). Just like softmax, we need the full Jacobian!

### The Forward Pass

Recall the layer norm formula:

1. Compute mean: $\mu = \frac{1}{d} \sum_i x_i$
2. Compute variance: $\sigma^2 = \frac{1}{d} \sum_i (x_i - \mu)^2$
3. Normalize: $\hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}$
4. Scale and shift: $y_i = \gamma_i \hat{x}_i + \beta_i$

### The Jacobian Problem

Every normalized output $\hat{x}_i$ depends on **all** inputs through $\mu$ and $\sigma$. This creates a full Jacobian matrix, not just diagonal elements.

### The Complete Backward Pass

The gradient computation requires carefully applying the chain rule through the normalization.

**Step 1a: Gradients for $\gamma$ and $\beta$**

These are straightforward:

$$
\frac{\partial L}{\partial \gamma_i} = \sum_{pos} \frac{\partial L}{\partial y_i[pos]} \cdot \hat{x}_i[pos]
$$

$$
\frac{\partial L}{\partial \beta_i} = \sum_{pos} \frac{\partial L}{\partial y_i[pos]}
$$

**Step 1b: Gradient through normalization**

For the normalized values:

$$
\frac{\partial L}{\partial \hat{x}_i} = \gamma_i \cdot \frac{\partial L}{\partial y_i}
$$

**Step 1c: The Jacobian through $\mu$ and $\sigma$**

This is where it gets interesting. The gradient with respect to the input is:

$$
\frac{\partial L}{\partial x_i} = \frac{1}{\sigma} \left( \frac{\partial L}{\partial \hat{x}_i} - \frac{1}{d}\sum_j \frac{\partial L}{\partial \hat{x}_j} - \hat{x}_i \cdot \frac{1}{d}\sum_j \hat{x}_j \frac{\partial L}{\partial \hat{x}_j} \right)
$$

This formula accounts for:
- The direct derivative through normalization
- The coupling through the mean (the $\frac{1}{d}\sum_j$ term)
- The coupling through the variance (the $\hat{x}_i \cdot \frac{1}{d}\sum_j$ term)

### Numerical Results

With the complete Jacobian:

```python
dL/dgamma: [ 0.1193, -0.1144, -0.1698,  0.0036, -0.0150,  0.4222,  0.0410, -0.0029,
            -0.0088,  0.0244, -0.0082, -0.0157,  0.0194, -0.0560,  0.0258,  0.0749]
dL/dbeta:  [ 0.1011,  0.2154,  0.1045,  0.0173,  0.0586, -0.3516, -0.0472,  0.0095,
            -0.0158, -0.0545,  0.0058,  0.0437, -0.0277, -0.0303,  0.0181, -0.2259]

dL/dresidual[0]: [ 0.0078,  0.6811,  0.3442, -0.4390,  0.4613, -2.4828,  1.6523,  1.3722,
                  -0.4716, -1.3366, -0.3174, -0.1642, -0.5059, -0.5174,  1.3532,  0.3627]
```

Notice how much larger these gradients are compared to a simple pass-through! The Jacobian properly accounts for how changing one input affects all outputs through the normalization statistics.

## Step 2: Backward Through Residual Connection

The residual connection is beautifully simple.

**Forward pass:**

$$
\text{residual} = \text{multi\_head\_output} + \text{ffn\_output}
$$

**Backward pass:**

When you have addition ($z = x + y$), the gradient splits equally to both inputs:

$$
\frac{\partial L}{\partial x} = \frac{\partial L}{\partial z}
$$

$$
\frac{\partial L}{\partial y} = \frac{\partial L}{\partial z}
$$

So the gradient from `residual` flows to **both** paths:

```python
dL/dmulti_head_output (from residual) = dL/dresidual
dL/dffn_output                        = dL/dresidual
```

This is why residual connections are so powerful for training deep networks — gradients flow directly through them without attenuation.

## Step 3: Backward Through FFN

Now we backpropagate through the feed-forward network. Remember the forward pass:

$$
\text{FFN}(x) = W_2 \cdot \text{GELU}(W_1 \cdot x + b_1) + b_2
$$

We work backward through three stages:

### Stage 3a: Backward Through Second Linear Layer

**Forward:**

$$
\text{ffn\_output} = W_2 \cdot \text{activated} + b_2
$$

**Gradients:**

$$
\frac{\partial L}{\partial W_2} = \sum_{pos} \frac{\partial L}{\partial \text{ffn\_output}[pos]} \otimes \text{activated}[pos]^T
$$

$$
\frac{\partial L}{\partial b_2} = \sum_{pos} \frac{\partial L}{\partial \text{ffn\_output}[pos]}
$$

$$
\frac{\partial L}{\partial \text{activated}} = W_2^T \cdot \frac{\partial L}{\partial \text{ffn\_output}}
$$

Results (sample):

```python
dL/dW2[0] (first 8 values): [ 0.0076,  0.0110,  0.0116,  0.0003,  0.0010,  0.0025, -0.0001,  0.0035, ...]
dL/db2: [ 0.1011,  0.2154,  0.1045,  0.0173,  0.0586, -0.3516, -0.0472,  0.0095, ...]
```

### Stage 3b: Backward Through GELU Activation

GELU is non-linear, so its gradient isn't constant. The derivative is:

$$
\text{GELU}'(x) = \frac{\partial}{\partial x}\left[0.5 \cdot x \cdot (1 + \tanh(\sqrt{2/\pi} \cdot (x + 0.044715 \cdot x^3)))\right]
$$

This is complex, but it's differentiable everywhere (unlike ReLU, which has a discontinuity at 0).

**Element-wise gradient:**

$$
\frac{\partial L}{\partial \text{hidden}[i]} = \frac{\partial L}{\partial \text{activated}[i]} \times \text{GELU}'(\text{hidden}[i])
$$

The GELU derivative is smooth and continuous, which helps with gradient flow.

### Stage 3c: Backward Through First Linear Layer

**Forward:**

$$
\text{hidden} = W_1 \cdot x + b_1
$$

**Gradients:**

$$
\frac{\partial L}{\partial W_1} = \sum_{pos} \frac{\partial L}{\partial \text{hidden}[pos]} \otimes x[pos]^T
$$

$$
\frac{\partial L}{\partial b_1} = \sum_{pos} \frac{\partial L}{\partial \text{hidden}[pos]}
$$

$$
\frac{\partial L}{\partial x} = W_1^T \cdot \frac{\partial L}{\partial \text{hidden}}
$$

This gives us gradients that flow back to the multi-head attention output:

```python
dL/dmulti_head_output (from FFN path): [-0.0455,  0.0873,  0.0265,  0.3470,  0.2680, -0.2172,
                                        -0.0797,  0.1364,  0.0728, -0.0719,  0.1163, -0.0594,
                                         0.0575,  0.1195,  0.1263,  0.0365]
```

## Step 4: Combine Gradients from Both Paths

Remember, the gradient flows through **two paths** to `multi_head_output`:

1. **Direct path** (residual): gradient passes through unchanged
2. **FFN path**: gradient passes through W1, GELU, W2

We **sum** these gradients:

$$
\frac{\partial L}{\partial \text{multi\_head\_output}} = \frac{\partial L}{\partial \text{multi\_head\_output}}_{residual} + \frac{\partial L}{\partial \text{multi\_head\_output}}_{FFN}
$$

```python
dL/dmulti_head_output[0] (total): [-0.0377,  0.7683,  0.3707, -0.0920,  0.7293, -2.7000,
                                    1.5726,  1.5086, -0.3988, -1.4085, -0.2011, -0.2236,
                                   -0.4484, -0.3979,  1.4795,  0.3992]
```

This is the gradient we'll use to backpropagate through multi-head attention.

Notice how much more significant these gradients are with the proper layer norm Jacobian! The accurate computation accounts for all dependencies, giving us the true gradient signal needed for learning.

## What We've Computed

Starting from $\frac{\partial L}{\partial \text{layer\_norm\_output}}$, we've computed **accurately**:

1. ✅ $\frac{\partial L}{\partial \gamma}$, $\frac{\partial L}{\partial \beta}$ — layer norm scale/shift parameters (with full Jacobian)
2. ✅ $\frac{\partial L}{\partial W_1}$, $\frac{\partial L}{\partial b_1}$ — gradients for first FFN layer
3. ✅ $\frac{\partial L}{\partial W_2}$, $\frac{\partial L}{\partial b_2}$ — gradients for second FFN layer
4. ✅ $\frac{\partial L}{\partial \text{multi\_head\_output}}$ — gradients to continue backprop

All computed with complete mathematical rigor. No approximations.

## Key Insights

**Layer Normalization Jacobian**: Just like softmax in attention, layer norm creates dependencies between all elements through mean and variance. The full Jacobian is essential for accurate gradients.

**Residual Connections**: Notice how the residual connection allows gradients to flow **directly** from the output back to the attention layer, bypassing the FFN entirely.

Without residuals, all gradients would have to flow through the FFN (and its non-linearity), potentially vanishing or exploding. With residuals, there's always a clean gradient path.

This is why residual connections are essential for training deep networks. They provide gradient highways.

## What's Next

We've made it through the FFN and layer norm. Now comes the most complex part of backpropagation:

**Multi-head attention.**

Attention involves:
- Query, key, value projections
- Scaled dot-product attention (with softmax)
- Multiple heads
- Output projection

Each of these needs gradients. It's intricate, but we'll work through it systematically.

Let's go.
