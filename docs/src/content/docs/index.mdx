---
title: Manual LLM Calculation
description: A complete forward and backward pass through a transformer, calculated by hand
template: splash
hero:
  tagline: Understanding transformers from first principles by manually calculating every step of training
  actions:
    - text: Start Calculating
      link: /tokenization/
      icon: right-arrow
      variant: primary
---

import { Card, CardGrid } from '@astrojs/starlight/components';

## Introduction

Ever wonder how transformers actually work under the hood? I mean really work, at the level of matrices and gradients and actual numbers?

Me too.

This is an educational project where we're going to manually calculate a complete training step through a tiny transformer model. And when I say "manually," I mean we're going to compute every single operation by hand (okay, with Python doing the arithmetic, because I'm not a masochist).

Here's what we're working with:

- **Model size:** 16-dimensional embeddings, 2 attention heads (tiny enough to be tractable)
- **Input:** One short text sequence
- **Process:** Tokenization → Embeddings → Attention → Feed-Forward → Loss → Gradients → Weight Updates

By the end, you'll have seen every matrix multiplication, every softmax, every gradient calculation. The goal is to build a deep, visceral understanding of how these models work at the mathematical level.

## What We'll Calculate

<CardGrid>
  <Card title="Forward Pass" icon="forward">
    Transform input text into predictions through embeddings, attention, and feed-forward layers
  </Card>

  <Card title="Backward Pass" icon="left-arrow">
    Calculate gradients for every parameter using backpropagation
  </Card>

  <Card title="Optimization" icon="setting">
    Update weights using the AdamW optimizer
  </Card>
</CardGrid>

## Architecture

We're using a simplified GPT-style decoder-only transformer (the same architecture family as ChatGPT, just... way tinier):

- **Vocabulary size:** Small (for illustration)
- **d_model:** 16 (embedding dimension)
- **Attention heads:** 2
- **d_ff:** 64 (feed-forward hidden dimension)
- **Layers:** 1 (to keep calculations manageable)

Look, I know. A real transformer has billions of parameters across dozens of layers. But the math is exactly the same whether you have 16 dimensions or 4096. We're just keeping things small enough that we can actually see what's happening.

Ready? Let's dive in.
